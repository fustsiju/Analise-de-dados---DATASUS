# -*- coding: utf-8 -*-
"""datasus2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EtzParDKhk2-9CHTIBxq5wntZD-HNJni
"""

!pip install git+https://github.com/danicat/pysus.git
!pip install dbfread
!pip install umap-learn

from datasus import read_dbc
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
sns.set()
from sklearn.metrics import adjusted_rand_score

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

# lê arquivo dbc
df = pd.DataFrame(read_dbc("/content/drive/MyDrive/Datasets/Datasus/DNMG2020.dbc"))
#df = pd.DataFrame(read_dbc("/content/drive/MyDrive/dataset/Datasus/DNMG2020.dbc"))

# novo df com as variáveis relevantes
dados = pd.DataFrame(columns=[])

df.columns

print(len(df))

#tratando dados ausentes
df.loc[df['DTNASC'] == '', 'DTNASC'] = 0 #<-- VALOR NÃO INFORMADO
#racacor de nenem cinzas?
df.loc[df['RACACOR'] == '', 'RACACOR'] = 6 #<-- VALOR NÃO INFORMADO
#NENENS VENTOS
df.loc[df['PESO'] == '', 'PESO'] = 0 #<-- VALOR NÃO INFORMADO
#ESCOLARIDADE VAZIA
df.loc[df['ESCMAE2010'] == '', 'ESCMAE2010'] = 9 #<-- VALOR PARA "IGNORADA"
#maes cinzas
df.loc[df['RACACORMAE'] == '', 'RACACORMAE'] = 6 #<-- VALOR PARA NÃO INFORMADO
#idadepai
df.loc[df['IDADEPAI'] == '', 'IDADEPAI'] = 0 #<-- SEM PAI PRESENTE
df.loc[df['IDADEPAI'] != 0, 'IDADEPAI'] = 1 #<-- COM PAI PRESENTE
#quantidade gestações
df.loc[df['QTDGESTANT'] == '', 'QTDGESTANT'] = 0
#consultas
df.loc[df['CONSULTAS'] == '', 'CONSULTAS'] = 9 #<-- VALOR PARA "IGNORADA"
#situação conjugal
df.loc[df['ESTCIVMAE'] == '', 'ESTCIVMAE'] = 9 #<-- VALOR PARA "IGNORADA"
#prenat
df.loc[df['MESPRENAT'] == '', 'MESPRENAT'] = 99 #<-- VALOR PARA "IGNORADA"
#parto
df.loc[df['PARTO'] == '', 'PARTO'] = 9 #<-- VALOR PARA "IGNORADA"

#atribuindo valores de df para dados
#dados['DTNASC'] = df['DTNASC']
dados['RACACOR'] = df['RACACOR']
#dados['PESO'] = df['PESO']
dados['ESCMAE2010'] = df['ESCMAE2010']
#dados['IDADEMAE'] = df['IDADEMAE']
dados['RACACORMAE'] = df['RACACORMAE']
#dados['IDADEPAI'] = df['IDADEPAI']
#dados['QTDGESTANT'] = df['QTDGESTANT']
#dados['CONSULTAS'] = df['CONSULTAS']
dados['ESTCIVMAE'] = df['ESTCIVMAE']
#dados['MESPRENAT'] = df['MESPRENAT']
dados['PARTO'] = df['PARTO']

#converter dados para float <-- evitar cannot convert string to float: ''
#dados['DTNASC'] = dados['DTNASC'].astype(float)
dados['RACACOR'] = dados['RACACOR'].astype(float)
#dados['PESO'] = dados['PESO'].astype(float)
dados['ESCMAE2010'] = dados['ESCMAE2010'].astype(float)
#dados['IDADEMAE'] = dados['IDADEMAE'].astype(float)
dados['RACACORMAE'] = dados['RACACORMAE'].astype(float)
#dados['IDADEPAI'] = dados['IDADEPAI'].astype(float)
#dados['QTDGESTANT'] = dados['QTDGESTANT'].astype(float)
#dados['CONSULTAS'] = dados['CONSULTAS'].astype(float)
dados['ESTCIVMAE'] = dados['ESTCIVMAE'].astype(float)
#dados['MESPRENAT'] = dados['MESPRENAT'].astype(float)
dados['PARTO'] = dados['PARTO'].astype(float)

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)


for i in range(1, 3):
  print(dados.iloc[i])

print(dados.size)
# Após imprimir, você pode redefinir as opções para o valor padrão, se necessário
pd.reset_option('display.max_columns')
pd.reset_option('display.max_rows')

dados_clusterizacao = dados

# Transformação de dados usando scaler
scaler = StandardScaler()
scaledData = scaler.fit_transform(dados)
print(scaledData)
print(len(scaledData))

pca = PCA(n_components=2) #<--- para gráfico bidimensional
pca.fit(scaledData)

scores_pca = pca.transform(scaledData)

#definindo o melhor valor de clusters
def calculate_wcss(scaledData):
    wcss = []
    for n in range(2, 21):
        kmeans = KMeans(n_clusters=n, random_state=42)
        kmeans.fit(X=dados)
        wcss.append(kmeans.inertia_)

    return wcss


def optimal_number_of_clusters(wcss):
    x1, y1 = 2, wcss[0]
    x2, y2 = 20, wcss[len(wcss) - 1]

    distances = []
    for i in range(len(wcss)):
        x0 = i + 2
        y0 = wcss[i]
        numerator = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2 * y1 - y2 * x1)
        denominator = np.sqrt((y2 - y1) ** 2 + (x2 - x1) ** 2)
        distances.append(numerator / denominator)

    return distances.index(max(distances)) + 2

k = optimal_number_of_clusters(calculate_wcss(scaledData))

k

wcs = []
for i in range(1,21):
  kmeans_pca = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
  kmeans_pca.fit(scores_pca)
  wcs.append(kmeans_pca.inertia_)

plt.figure(figsize = (10,8))
plt.plot(range(1,21), wcs, marker = 'o', linestyle = '--')
plt.xlabel('Numero de clusters')
plt.ylabel('wcs')
plt.title('Método Cotovelo')
plt.show()

kmeans_pca = KMeans(n_clusters = k, init = 'k-means++', random_state = 42, max_iter=600)
kmeans_pca.fit(scores_pca)

df_segm_pca_kmeans = pd.concat([dados.reset_index(drop = True), pd.DataFrame(scores_pca)], axis = 1)
df_segm_pca_kmeans.columns.values[-2:] = ['Linha X', 'Linha Y']

df_segm_pca_kmeans['Segmentos A'] = kmeans_pca.labels_

df_segm_pca_kmeans.head()

ari = adjusted_rand_score(df_segm_pca_kmeans['Segmentos A'], kmeans_pca.labels_)

print(f"Índice de Rand Ajustado: {ari}")

from sklearn.metrics import silhouette_score

silhueta = silhouette_score(df_segm_pca_kmeans, kmeans_pca.labels_)

print(silhueta)

from sklearn.metrics import davies_bouldin_score

davies_bouldin = davies_bouldin_score(df_segm_pca_kmeans, kmeans_pca.labels_)
print(davies_bouldin)

df_segm_pca_kmeans['Segmentos A'].value_counts()

df_segm_pca_kmeans['Clusters'] = df_segm_pca_kmeans['Segmentos A'].map({1: 'Cluster A', 2: 'Cluster B', 6: 'Cluster C', 4: 'Cluster D', 0: 'Cluster E', 3: 'Cluster F', 4: 'Cluster G', 5: 'Cluster H'})

df_segm_pca_kmeans['Clusters'].value_counts()

print(len(df_segm_pca_kmeans))

plt.figure(figsize=(10, 6))


sns.scatterplot(x=df_segm_pca_kmeans['Linha X'], y=df_segm_pca_kmeans['Linha Y'], hue=df_segm_pca_kmeans['Clusters'], s=100, alpha=0.5) #<-- falar sobre o alpha e a quantidade de sobreposição / minimo 0.002

ax = plt.gca()

centroids = kmeans_pca.cluster_centers_
centroid_scatter = ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=150, alpha=1)
# Scatter plot dos centróides
centroids = kmeans_pca.cluster_centers_
# Ajuste os limites do eixo x e y para dar zoom
plt.xlim(-4, 4)
plt.ylim(-4, 6)

plt.title("Clusters")
plt.show()

fig = sns.displot(data = df_segm_pca_kmeans, x = df_segm_pca_kmeans['RACACORMAE'], hue=df_segm_pca_kmeans['Clusters'], multiple="stack")
plt.xlim(0, 6.2)

plt.title("Tipos de raça")
plt.show()

for i in range(1,7):
  fig = sns.displot(data = df_segm_pca_kmeans, x = df_segm_pca_kmeans['RACACORMAE'], hue=df_segm_pca_kmeans['Segmentos A'] == i, multiple="stack")
  plt.title(f'Cluster {i}')

df_segm_pca_kmeans['RACACORMAE'].value_counts()

df_proportions_racacormae = df_segm_pca_kmeans['RACACORMAE'].value_counts(normalize=True) * 100
df_proportions_racacormae

df_segm_pca_kmeans['ESTCIVMAE'].value_counts()

fig = sns.displot(data = df_segm_pca_kmeans, x = df_segm_pca_kmeans['ESTCIVMAE'], hue=df_segm_pca_kmeans['Clusters'], legend=True)

plt.title("Estado cívil")
plt.show()

df_segm_pca_kmeans['PARTO'].value_counts()

df_segm_pca_kmeans['ESCMAE2010'].value_counts()

df_proportions_escmae = df_segm_pca_kmeans['ESCMAE2010'].value_counts(normalize=True) * 100
df_proportions_escmae
